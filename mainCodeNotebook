{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FehW8ewJz1HO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
            "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "import gdown\n",
        "import timm  # Import timm for Vision Transformer models\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F  # Import torch.nn.functional as F\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torch.optim import lr_scheduler  # Import lr_scheduler correctly\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "device = \"cuda:1\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWSvFaaU0Pof",
        "outputId": "1769e419-51c4-4e92-ba04-4d3a01ec3fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IEnpbGjNqXYF4vPY1NW-ODcrYZomyb4S\n",
            "From (redirected): https://drive.google.com/uc?id=1IEnpbGjNqXYF4vPY1NW-ODcrYZomyb4S&confirm=t&uuid=cffc64e7-1736-4b68-b64a-73d6b3fe384b\n",
            "To: /home/atellezfernandez/git/Juridique/COMPVIZ/data/dataset.zip\n",
            " 27%|██▋       | 82.3M/310M [00:00<00:01, 116MB/s] "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/uc?id=1IEnpbGjNqXYF4vPY1NW-ODcrYZomyb4S\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/dataset.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Unzip dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip -q data/dataset.zip -d data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/gdown/download.py:368\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    366\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39mtotal, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial\u001b[38;5;241m=\u001b[39mstart_size, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    367\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mCHUNK_SIZE):\n\u001b[1;32m    369\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/urllib3/response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 936\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    939\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/http/client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = \"cuda:1\"\n",
        "print(device)\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download dataset using gdown\n",
        "url = 'https://drive.google.com/uc?id=1IEnpbGjNqXYF4vPY1NW-ODcrYZomyb4S'\n",
        "output = 'data/dataset.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Unzip dataset\n",
        "!unzip -q data/dataset.zip -d data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B732qPLYtD_E"
      },
      "source": [
        "A class BirdDataset is defined in order to load all the data from the storage directory (path), the path to the class mapping file (class_mapping_path) and a few transforms to be applied to the images (transforms)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dYMFPhkA0Rre"
      },
      "outputs": [],
      "source": [
        "# Define BirdDataset\n",
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, path, class_mapping_path, transforms=None):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        self.image_path = list(Path(path).glob(\"*/*.jpg\"))\n",
        "        self.class_mapping = pd.read_csv(class_mapping_path)\n",
        "        self.classes = self.class_mapping.sort_values(by='idx').category_cub.to_list()\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_path[idx]\n",
        "        target = torch.tensor(self.classes.index(image_path.parent.name))\n",
        "        with open(image_path, 'rb') as f:\n",
        "            with Image.open(f) as img:\n",
        "                image = img.convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "        return image, target"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zDhCEYWWtD_E"
      },
      "source": [
        "Data augmentation is carried out on the training data and the validation data in order to diversify our data and make it more general and therefore more robust. This will help to reduce overfitting later on. More specifically, functions that will allow us to resize, randomly crop (randomresizedcrop) or perform random horizontal inversion (RandomHorizontalFlip).  ----- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YpxBRgw_0TkT"
      },
      "outputs": [],
      "source": [
        "#EASY TRANSFORMATION!! MOST PERFORMANT ONE\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),  # Redimensionner le plus petit côté à 256 pixels\n",
        "    transforms.RandomResizedCrop(224, interpolation=InterpolationMode.BICUBIC),  # Recadrer aléatoirement à 224x224 pixels\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomRotation(30),  # Augmenter la rotation aléatoire\n",
        "    #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "    #transforms.RandomGrayscale(p=0.2),  # Ajouter du RandomGrayscale pour augmenter la variété des données\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "#The descomented transformations are the ones that are a bit more complex, they are worst\n",
        "\n",
        "# Data augmentations and transformations for validation\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I propose AugMix, a data processing technique that mixes augmented images and enforces consistent embeddings of the augmented images, which results in increased robustness and improved uncertainty calibration. AugMix does not require tuning to work correctly, as with random cropping or CutOut, and thus enables plug-and-play data augmentation. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance by more than half in some cases.\n",
        "\n",
        "- CLASS AugMixTransform :  custom Albumentations transformation that wraps the augment_and_mix function to integrate it into the transformation pipeline\n",
        "\n",
        "- class AlbumentationsTransformWrapper : wrapper to make Albumentations transformations compatible with torchvision transformations.\n",
        "\n",
        "- data_transforms_train_heavy: REALLY HEAVY TRANSFORMATIONS (random cropping, rotation, noise, optical distortion, etc.) + AugMix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import random\n",
        "import albumentations as A\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def augmentations_list():\n",
        "    return [\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
        "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),\n",
        "        A.Solarize(p=1.0),\n",
        "        A.Posterize(num_bits=4, p=1.0),\n",
        "        A.Equalize(p=1.0),\n",
        "        A.InvertImg(p=1.0),\n",
        "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0),\n",
        "        A.Emboss(alpha=(0.2, 0.5), strength=(0.5, 1.0), p=1.0)\n",
        "    ]\n",
        "\n",
        "def normalize(image, mean, std):\n",
        "    \"\"\"Normalize input image channel-wise to zero mean and unit variance.\"\"\"\n",
        "    image = image.transpose(2, 0, 1).astype(np.float32)  # Switch to channel-first and convert to float32\n",
        "    mean, std = np.array(mean), np.array(std)\n",
        "    image = (image - mean[:, None, None]) / std[:, None, None]\n",
        "    return image.transpose(1, 2, 0).astype(np.float32)\n",
        "\n",
        "def apply_op(image, op):\n",
        "    image = np.clip(image * 255., 0, 255).astype(np.uint8)\n",
        "    pil_img = Image.fromarray(image)  # Convert to PIL.Image\n",
        "    pil_img = op(image=np.array(pil_img))['image']\n",
        "    return np.asarray(pil_img) / 255.\n",
        "\n",
        "def augment_and_mix(image, severity=3, width=3, depth=-1, alpha=1., mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"Perform AugMix augmentations and compute mixture.\n",
        "\n",
        "    Args:\n",
        "        image: Raw input image as float32 np.ndarray of shape (h, w, c)\n",
        "        severity: Severity of underlying augmentation operators (between 1 to 10).\n",
        "        width: Width of augmentation chain\n",
        "        depth: Depth of augmentation chain. -1 enables stochastic depth uniformly\n",
        "            from [1, 3]\n",
        "        alpha: Probability coefficient for Beta and Dirichlet distributions.\n",
        "\n",
        "    Returns:\n",
        "        mixed: Augmented and mixed image.\n",
        "    \"\"\"\n",
        "    ws = np.float32(\n",
        "        np.random.dirichlet([alpha] * width))\n",
        "    m = np.float32(np.random.beta(alpha, alpha))\n",
        "\n",
        "    mix = np.zeros_like(image, dtype=np.float32)\n",
        "    ops = augmentations_list()\n",
        "    for i in range(width):\n",
        "        image_aug = image.copy().astype(np.float32)\n",
        "        d = depth if depth > 0 else np.random.randint(1, 4)\n",
        "        for _ in range(d):\n",
        "            op = random.choice(ops)\n",
        "            image_aug = apply_op(image_aug, op)\n",
        "        # Preprocessing commutes since all coefficients are convex\n",
        "        mix += ws[i] * normalize(image_aug, mean, std)\n",
        "\n",
        "    mixed = (1 - m) * normalize(image, mean, std) + m * mix\n",
        "    return mixed\n",
        "\n",
        "# Custom Albumentations transformation for AugMix\n",
        "class AugMixTransform(A.ImageOnlyTransform):\n",
        "    def __init__(self, always_apply=False, p=1.0):\n",
        "        super(AugMixTransform, self).__init__(always_apply, p)\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def apply(self, img, **params):\n",
        "        return augment_and_mix(img, mean=self.mean, std=self.std)\n",
        "\n",
        "# Wrapper class to make Albumentations compatible with torchvision transforms\n",
        "class AlbumentationsTransformWrapper:\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = np.array(img)\n",
        "        transformed = self.transform(image=img)\n",
        "        return transformed[\"image\"]\n",
        "\n",
        "# Define your heavy data transformations including Albumentations and AugMix\n",
        "data_transforms_train_heavy = AlbumentationsTransformWrapper(A.Compose([\n",
        "    A.RandomResizedCrop(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.OneOf([\n",
        "        A.GaussNoise(),\n",
        "        A.ISONoise(),\n",
        "    ], p=0.2),\n",
        "    A.OneOf([\n",
        "        A.MotionBlur(p=0.2),\n",
        "        A.MedianBlur(blur_limit=3, p=0.1),\n",
        "        A.Blur(blur_limit=3, p=0.1),\n",
        "    ], p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.2),\n",
        "    A.OneOf([\n",
        "        A.OpticalDistortion(p=0.3),\n",
        "        A.GridDistortion(p=0.1),\n",
        "        A.PiecewiseAffine(p=0.3),\n",
        "    ], p=0.2),\n",
        "    A.OneOf([\n",
        "        A.Sharpen(p=1.0),\n",
        "        A.Emboss(p=1.0),\n",
        "        A.RandomBrightnessContrast(p=1.0),            \n",
        "    ], p=0.3),\n",
        "    A.OneOf([\n",
        "        A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=10, val_shift_limit=10, p=0.1),\n",
        "        A.ColorJitter(brightness=0.3, contrast=0.1, saturation=0.1, hue=0.1, p=0.1),\n",
        "    ]),\n",
        "    AugMixTransform(p=1.0),\n",
        "    A.Resize(224, 224, interpolation=cv2.INTER_LANCZOS4),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "]))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- On décide de prendre un batch_size de 32 après avoir essayer de prendre d'autres tailles de batch tel que 16 et 64.\n",
        "- On a divisé notre dataset en train set et en validation set\n",
        "- On a décidé d'utiliser les dataloader car ils permettent de diviser automatiquement le jeu de données en lots (batches) de taille spécifiée. Cela simplifie le code et évite d'avoir à implémenter manuellement cette logique. En plus, ils nous permettent de mélanger les données pour réduire l'overfitting lors de l'entrainement du modèle. Ils permettent aussi un chargemnt efficace des données.\n",
        "\n",
        "(This was chosen on the first model we did)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VAblGHfv0VE7"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = BirdDataset(\"./data/dataset/cropped_train_images/\", \"./data/dataset/class_indexes.csv\", transforms=data_transforms_train_heavy)\n",
        "val_dataset = BirdDataset(\"./data/dataset/cropped_val_images/\", \"./data/dataset/class_indexes.csv\", transforms=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F  # Import torch.nn.functional as F\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "nclasses = len(train_dataset.classes)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After testing various models such as ConvNext, ResNet30, ResNet50, EfficientNet, vit_base, and VAE, we decided to retain the vit_large_patch16_224 model due to its superior accuracy. To implement this, the VitNet class was created using PyTorch Lightning. This class inherits from pl.LightningModule, facilitating clear separation of training, validation, and prediction code, thereby enhancing readability and organization.\n",
        "\n",
        "### Class: VitNet\n",
        "- **Inherits from:** pl.LightningModule\n",
        "- **Base model:** Pre-trained Vision Transformer (ViT)\n",
        "\n",
        "### Functions:\n",
        "\n",
        "1. **forward(x)**\n",
        "   - **Purpose:** Defines the data flow through the model.\n",
        "   - **Process:**\n",
        "     1. Input data 'x' passes through the ViT model.\n",
        "     2. Non-linearity applied using ReLU.\n",
        "     3. Data passes through a linear layer to obtain final predictions.\n",
        "\n",
        "2. **training_step(batch, batch_idx)**\n",
        "   - **Purpose:** Calculates the loss during training.\n",
        "   - **Process:**\n",
        "     1. Compute loss using cross-entropy on predictions.\n",
        "     2. Store intermediate results (loss, correct predictions, total targets) in train_outputs list.\n",
        "   - **Reason:** Cross-entropy is suitable for multi-class classification.\n",
        "\n",
        "3. **on_train_epoch_end()**\n",
        "   - **Purpose:** Calculates average training loss and accuracy at the end of each epoch.\n",
        "   - **Process:**\n",
        "     1. Calculate average loss from train_outputs.\n",
        "     2. Compute training accuracy.\n",
        "     3. Clear train_outputs list for the next epoch.\n",
        "\n",
        "4. **validation_step(batch, batch_idx)**\n",
        "   - **Purpose:** Validates the model.\n",
        "   - **Process:** Similar to training_step, but stores results in val_outputs.\n",
        "\n",
        "5. **on_validation_epoch_end()**\n",
        "   - **Purpose:** Calculates average validation loss and accuracy at the end of each epoch.\n",
        "   - **Process:** Similar to on_train_epoch_end, but uses validation results.\n",
        "\n",
        "6. **configure_optimizers()**\n",
        "   - **Purpose:** Configures the optimizer and learning rate scheduler.\n",
        "   - **Process:**\n",
        "     1. Use SGD optimizer (preferred over AdamW for better results).\n",
        "     2. Apply CosineAnnealingLR to update the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VitNet(pl.LightningModule):\n",
        "    def __init__(self, lr, momentum):\n",
        "        super(VitNet, self).__init__()\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.vit = timm.create_model('vit_large_patch16_224', pretrained=True)\n",
        "        num_ftrs = self.vit.head.in_features\n",
        "        self.vit.head = nn.Identity()\n",
        "        self.fc = nn.Linear(num_ftrs, nclasses)\n",
        "        self.train_outputs = []\n",
        "        self.val_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.vit(x))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, target = batch\n",
        "        output = self(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, target)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        n_correct_pred = pred.eq(target.data.view_as(pred)).sum().detach()\n",
        "        self.train_outputs.append({'loss': loss, 'n_correct_pred': n_correct_pred, 'n_pred': len(target)})\n",
        "        self.log('train_loss', loss)  # log the training loss\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_loss = torch.stack([x['loss'] for x in self.train_outputs]).mean()\n",
        "        train_acc = sum([x['n_correct_pred'] for x in self.train_outputs]) / sum(x['n_pred'] for x in self.train_outputs)\n",
        "        print(f\"Epoch {self.current_epoch}: Train Loss: {avg_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "        self.train_outputs.clear()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data, target = batch\n",
        "        output = self(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(output, target)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        n_correct_pred = pred.eq(target.data.view_as(pred)).sum().detach()\n",
        "        self.val_outputs.append({'val_loss': loss, 'n_correct_pred': n_correct_pred, 'n_pred': len(target)})\n",
        "        self.log('val_loss', loss)  # log the validation loss\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in self.val_outputs]).mean()\n",
        "        val_acc = sum([x['n_correct_pred'] for x in self.val_outputs]) / sum(x['n_pred'] for x in self.val_outputs)\n",
        "        print(f\"Epoch {self.current_epoch}: Val Loss: {avg_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "        self.val_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Callbacks in PyTorch Lightning\n",
        "- **Purpose:** Add custom behaviors during training.\n",
        "\n",
        "#### Model Checkpoint\n",
        "- **Why:** Save the model and its hyperparameters when it achieves the lowest validation loss.\n",
        "- **Benefits:**\n",
        "  - Captures the best state of the model before overfitting.\n",
        "  - Allows comparison of different model versions to assess changes in hyperparameters or structure.\n",
        "\n",
        "#### Early Stopping\n",
        "- **Criteria:** Stops training after 5 epochs without improvement (no reduction in validation loss).\n",
        "\n",
        "### Model Training and Validation\n",
        "- **Tool:** PyTorch Lightning Trainer\n",
        "- **Training Duration:** Maximum of 50 epochs\n",
        "\n",
        "This schematic approach highlights the use of callbacks and the trainer to optimize the training and validation process effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "ename": "DeferredCudaCallError",
          "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n    app.start()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n    self.io_loop.start()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n    self._run_once()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n    handle._run()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n    await self.process_one()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n    await dispatch(*args)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n    await result\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n    reply_content = await reply_content\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n    res = shell.run_cell(\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n    result = self._run_cell(\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n    result = runner(coro)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_938251/2519443962.py\", line 2, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:306\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:174\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 174\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:448\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     27\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, \n\u001b[1;32m     28\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:938\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    937\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: setting up strategy environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 938\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__setup_profiler()\n\u001b[1;32m    941\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_setup_hook(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# allow user to setup lightning_module in accelerator environment\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:129\u001b[0m, in \u001b[0;36mStrategy.setup_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Setup any processes or distributed connections.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mThis is called before the LightningModule/DataModule setup hook which allows the user to access the accelerator\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03menvironment before setup is complete.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py:44\u001b[0m, in \u001b[0;36mCUDAAccelerator.setup_device\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice should be GPU, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43m_check_cuda_matmul_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(device)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py:349\u001b[0m, in \u001b[0;36m_check_cuda_matmul_precision\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TORCH_GREATER_EQUAL_1_12:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# before 1.12, tf32 was used by default\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m major, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m ampere_or_later \u001b[38;5;241m=\u001b[39m major \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# Ampere and later leverage tensor cores, where this setting becomes useful\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ampere_or_later:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py:312\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(orig_traceback)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             )\n\u001b[0;32m--> 312\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n    app.start()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n    self.io_loop.start()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n    self._run_once()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n    handle._run()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n    await self.process_one()\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n    await dispatch(*args)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n    await result\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n    reply_content = await reply_content\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n    res = shell.run_cell(\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n    result = self._run_cell(\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n    result = runner(coro)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_938251/2519443962.py\", line 2, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/atellezfernandez/.pyenv/versions/3.10.0/envs/theraclion/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n"
          ]
        }
      ],
      "source": [
        "# Training and validation\n",
        "from torch.optim import lr_scheduler  # Import lr_scheduler correctly\n",
        "lr = 0.01\n",
        "momentum = 0.89\n",
        "\n",
        "model = VitNet(lr, momentum)\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',  # Change this line\n",
        "    dirpath='checkpoints',\n",
        "    filename='best_checkpoint_MIX_new_new',\n",
        "    save_top_k=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',  # Change this line\n",
        "    patience=5,\n",
        "    verbose=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,       ##################################### that is where i call the hard stopping!!!\n",
        "    #i forced to be 10\n",
        "    devices=1 if torch.cuda.is_available() else 0, \n",
        "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "    precision=16,  # Enable Automatic Mixed Precision\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    gradient_clip_val=0.5  # Gradient clipping\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'best_model_yolo3_MIX.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 620/620 [00:07<00:00, 79.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "def predict_test(model, test_dir, output_path, transform):\n",
        "    model.eval()\n",
        "    output_file = open(output_path, \"w\")\n",
        "    output_file.write(\"ID,category\\n\")\n",
        "    for f in tqdm(os.listdir(test_dir)):\n",
        "        if 'jpg' in f:\n",
        "            data = transform(pil_loader(os.path.join(test_dir, f)))\n",
        "            data = data.unsqueeze(0).to(device)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            output_file.write(\"%s,%d\\n\" % (f[:-4], pred.item()))\n",
        "    output_file.close()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model_yolo3_MIX_new_new.pth'))\n",
        "model.to(device)\n",
        "\n",
        "# Set the test directory and output path\n",
        "test_dir = './data/dataset/test_images'\n",
        "output_path = './yooyo_kaggle_vit_large_yolo3_MIX_new_new.csv'\n",
        "\n",
        "\n",
        "# Make predictions on the test data\n",
        "predict_test(model, test_dir, output_path, val_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e09a2fe985d44caa86877b0e5976578": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0227d87c6c40b9bd1f5f09ba4d4906": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59a8928d824243388c9d7546cfd23ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_744fdf3f3882441984a18c84f7c0f265",
              "IPY_MODEL_db6ab2eead794460920ec04d1d666918",
              "IPY_MODEL_83a13b108b39403baf3f47a815ab604e"
            ],
            "layout": "IPY_MODEL_810ca76d074e478e97d71af66fe714f2"
          }
        },
        "6fcb38ebb2ff4ad1ac8a052e1e1baa69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "744fdf3f3882441984a18c84f7c0f265": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ffe80a25d946f3a70deacf7fb4d80e",
            "placeholder": "​",
            "style": "IPY_MODEL_7b665bbd203b4c96beaf0c7eaed45d7e",
            "value": "model.safetensors: 100%"
          }
        },
        "7b665bbd203b4c96beaf0c7eaed45d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "810ca76d074e478e97d71af66fe714f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a13b108b39403baf3f47a815ab604e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e09a2fe985d44caa86877b0e5976578",
            "placeholder": "​",
            "style": "IPY_MODEL_a0e447b767ef4d65a31d6d231defb333",
            "value": " 1.22G/1.22G [00:14&lt;00:00, 138MB/s]"
          }
        },
        "a0e447b767ef4d65a31d6d231defb333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db6ab2eead794460920ec04d1d666918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fcb38ebb2ff4ad1ac8a052e1e1baa69",
            "max": 1217334682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f0227d87c6c40b9bd1f5f09ba4d4906",
            "value": 1217334682
          }
        },
        "f6ffe80a25d946f3a70deacf7fb4d80e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
